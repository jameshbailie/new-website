---
title: "Generalization bounds and stopping rules for learning with self-selected data"
collection: papers
category: foundations
permalink: /papers/2025-05-12-Generalization-bounds-and-stopping-rules-for-learning-with-self-selected-data
date: 2025-05-12
venue: 'Preprint'
authors_short: 'J Rodemann, !!me!!'
authors_long: 'Julian Rodemann, !!me!!'
citation: 'Julian Rodemann and James Bailie (2025). “Generalization Bounds and Stopping Rules for Learning with Self-Selected Data”. doi: <a href="https://doi.org/10.48550/arXiv.2505.07367" target="_blank">10.48550/arXiv.2505.07367</a>'
abstract: "Many learning paradigms self-select training data in light of previously learned parameters. Examples include active learning, semi-supervised learning, bandits, or boosting. Rodemann et al. (2024) unify them under the framework of 'reciprocal learning'. In this article, we address the question of how well these methods can generalize from their self-selected samples. In particular, we prove universal generalization bounds for reciprocal learning using covering numbers and Wasserstein ambiguity sets. Our results require no assumptions on the distribution of self-selected data, only verifiable conditions on the algorithms. We prove results for both convergent and finite iteration solutions. The latter are anytime valid, thereby giving rise to stopping rules for a practitioner seeking to guarantee the out-of-sample performance of their reciprocal learning algorithm. Finally, we illustrate our bounds and stopping rules for reciprocal learning's special case of semi-supervised learning."
bibtex_url: 'true'
preprint_url: 'true'
arxiv_url: 'http://arxiv.org/abs/2505.07367'
---
